{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ab0f77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T10:45:31.547517Z",
     "iopub.status.busy": "2026-01-08T10:45:31.547195Z",
     "iopub.status.idle": "2026-01-08T10:45:31.892024Z",
     "shell.execute_reply": "2026-01-08T10:45:31.891772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw CSV loaded: (558, 278)\n",
      "\n",
      "Found 4 sets of duplicate columns in raw CSV:\n",
      "\n",
      "  'Q33_Which of the following Yetagon products and services have you used ? (Please'...\n",
      "    Column at index 183 ('Q33_Which of the following Yetagon products and services have you used ? (Please select all that apply.)/Yetagon Tele-agronomy'): 529 non-null, 0 ones\n",
      "    Column at index 184 ('Q33_Which of the following Yetagon products and services have you used ? (Please select all that apply.)/Yetagon Tele-agronomy.1'): 529 non-null, 264 ones\n",
      "    → Keeping column at index 184 (264 ones, 529 non-null)\n",
      "\n",
      "  'Direct or Digital?'...\n",
      "    Column at index 3 ('Direct or Digital?'): 558 non-null, 0 ones\n",
      "    Column at index 264 ('Direct or Digital?.1'): 0 non-null, 0 ones\n",
      "    → Keeping column at index 3 (0 ones, 558 non-null)\n",
      "\n",
      "  'Q26_[Confident] [other]  How do they use the Facebook? OR how confident are they'...\n",
      "    Column at index 145 ('Q26_[Confident] [other]  How do they use the Facebook? OR how confident are they using others?'): 13 non-null, 0 ones\n",
      "    Column at index 265 ('Q26_[Confident] [other]  How do they use the Facebook? OR how confident are they using others?.1'): 0 non-null, 0 ones\n",
      "    → Keeping column at index 145 (0 ones, 13 non-null)\n",
      "\n",
      "  'Q36_If other, please specify'...\n",
      "    Column at index 224 ('Q36_If other, please specify'): 404 non-null, 0 ones\n",
      "    Column at index 266 ('Q36_If other, please specify.1'): 558 non-null, 0 ones\n",
      "    → Keeping column at index 266 (0 ones, 558 non-null)\n",
      "\n",
      "After removing raw CSV duplicates: (558, 274)\n",
      "\n",
      "Found 1 duplicate column names after renaming\n",
      "  Processing 'Q37': 2 columns\n",
      "    Column at index 225: 529 non-null, 0 ones\n",
      "    Column at index 263: 0 non-null, 0 ones\n",
      "    → Keeping column at index 225 (0 ones, 529 non-null)\n",
      "After duplicate merging: 273 columns, 273 unique\n",
      "\n",
      "Found 15 Township columns\n",
      "Created Township column with 558 non-null values\n",
      "Positioned Township column after Region column\n",
      "Dropped 15 individual township columns\n",
      "\n",
      "Converted 176 binary columns to Boolean values\n",
      "Removed 12 metadata columns\n",
      "\n",
      "Created CLEAN_FY26.csv (558 rows, 247 columns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Read the CSV\n",
    "df = pd.read_csv('Digital_Digital_Access__Behavior_Survey_DAB_-_all_versions_-_English_eng_-_2025-12-29-02-53-07.csv', delimiter=';')\n",
    "\n",
    "print(f\"Raw CSV loaded: {df.shape}\")\n",
    "\n",
    "# FIRST: Handle duplicate columns from raw CSV (before any renaming)\n",
    "# Pandas adds .1, .2, .3 suffixes when reading duplicate column names\n",
    "import re as regex_lib\n",
    "duplicate_cols_in_raw = {}\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    # Check if this column has a .1, .2, etc suffix from pandas\n",
    "    if regex_lib.search(r'\\.\\d+$', col):\n",
    "        base_name = regex_lib.sub(r'\\.\\d+$', '', col)\n",
    "        if base_name not in duplicate_cols_in_raw:\n",
    "            duplicate_cols_in_raw[base_name] = []\n",
    "        duplicate_cols_in_raw[base_name].append((i, col))\n",
    "\n",
    "if duplicate_cols_in_raw:\n",
    "    print(f\"\\nFound {len(duplicate_cols_in_raw)} sets of duplicate columns in raw CSV:\")\n",
    "    \n",
    "    for base_name, cols in duplicate_cols_in_raw.items():\n",
    "        print(f\"\\n  '{base_name[:80]}'...\")\n",
    "        \n",
    "        # Find the base column too\n",
    "        base_indices = [i for i, c in enumerate(df.columns) if c == base_name]\n",
    "        all_cols = [(i, base_name) for i in base_indices] + cols\n",
    "        \n",
    "        # Find which column has the most 1s (actual data), not just non-null\n",
    "        best_idx = all_cols[0][0]\n",
    "        best_ones = 0\n",
    "        best_non_null = 0\n",
    "        \n",
    "        for idx, col_name in all_cols:\n",
    "            non_null = df.iloc[:, idx].notna().sum()\n",
    "            ones = (df.iloc[:, idx] == 1.0).sum() + (df.iloc[:, idx] == 1).sum()\n",
    "            \n",
    "            print(f\"    Column at index {idx} ('{col_name}'): {non_null} non-null, {ones} ones\")\n",
    "            \n",
    "            # Prioritize columns with 1s, then by non-null count\n",
    "            if ones > best_ones or (ones == best_ones and non_null > best_non_null):\n",
    "                best_idx = idx\n",
    "                best_ones = ones\n",
    "                best_non_null = non_null\n",
    "        \n",
    "        print(f\"    → Keeping column at index {best_idx} ({best_ones} ones, {best_non_null} non-null)\")\n",
    "        \n",
    "        # Keep the best column's data in the base column name\n",
    "        if best_idx != base_indices[0] if base_indices else -1:\n",
    "            # Copy data from best column to base column\n",
    "            if base_indices:\n",
    "                df[base_name] = df.iloc[:, best_idx]\n",
    "            else:\n",
    "                # Base column doesn't exist, rename the best one\n",
    "                df = df.rename(columns={df.columns[best_idx]: base_name})\n",
    "        \n",
    "        # Drop all suffix columns\n",
    "        for idx, col_name in cols:\n",
    "            if col_name in df.columns:\n",
    "                df = df.drop(columns=[col_name])\n",
    "\n",
    "print(f\"\\nAfter removing raw CSV duplicates: {df.shape}\")\n",
    "\n",
    "# NOW proceed with renaming\n",
    "rename_map = {}\n",
    "option_counters = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    # Special case: Q01.1 -> Q00\n",
    "    if col.startswith('Q01.1'):\n",
    "        rename_map[col] = 'Q00'\n",
    "        continue\n",
    "    \n",
    "    # Sub-question pattern: Q##[_-]text/option\n",
    "    if match := re.match(r'(Q\\d+)[_-](.*[?\\)\"])/(.*)', col):\n",
    "        q_code = match.group(1)\n",
    "\n",
    "        # Generate sub-code (Q10_a, Q10_b, etc.)\n",
    "        if q_code not in option_counters:\n",
    "            option_counters[q_code] = 0\n",
    "        sub_code = f\"{q_code}_{chr(97 + option_counters[q_code])}\"\n",
    "        option_counters[q_code] += 1\n",
    "\n",
    "        rename_map[col] = sub_code\n",
    "\n",
    "    # Follow-up pattern: Q##[_-]If other...\n",
    "    elif match := re.match(r'(Q\\d+)[_-](If .*)', col):\n",
    "        q_code = match.group(1)\n",
    "        follow_up_code = f\"{q_code}_other_text\"\n",
    "        rename_map[col] = follow_up_code\n",
    "\n",
    "    # Main question pattern: Q##[_-]text\n",
    "    elif match := re.match(r'(Q\\d+)[_-](.*)', col):\n",
    "        q_code = match.group(1)\n",
    "        rename_map[col] = q_code\n",
    "\n",
    "# Create cleaned version with codes\n",
    "df_cleaned = df.rename(columns=rename_map)\n",
    "\n",
    "# Handle any duplicate column names created by renaming (shouldn't happen now, but keep as safety)\n",
    "col_counts = Counter(df_cleaned.columns)\n",
    "dup_names = {k: v for k, v in col_counts.items() if v > 1}\n",
    "\n",
    "if dup_names:\n",
    "    print(f\"\\nFound {len(dup_names)} duplicate column names after renaming\")\n",
    "    for dup_name in dup_names.keys():\n",
    "        print(f\"  Processing '{dup_name}': {dup_names[dup_name]} columns\")\n",
    "        \n",
    "        dup_indices = [i for i, col in enumerate(df_cleaned.columns) if col == dup_name]\n",
    "        \n",
    "        # Find best column (most 1s, then most non-null)\n",
    "        best_idx = dup_indices[0]\n",
    "        best_ones = 0\n",
    "        best_non_null = 0\n",
    "        \n",
    "        for idx in dup_indices:\n",
    "            non_null = df_cleaned.iloc[:, idx].notna().sum()\n",
    "            ones = (df_cleaned.iloc[:, idx] == 1.0).sum() + (df_cleaned.iloc[:, idx] == 1).sum()\n",
    "            \n",
    "            print(f\"    Column at index {idx}: {non_null} non-null, {ones} ones\")\n",
    "            \n",
    "            if ones > best_ones or (ones == best_ones and non_null > best_non_null):\n",
    "                best_idx = idx\n",
    "                best_ones = ones\n",
    "                best_non_null = non_null\n",
    "        \n",
    "        print(f\"    → Keeping column at index {best_idx} ({best_ones} ones, {best_non_null} non-null)\")\n",
    "\n",
    "        # Keep best column\n",
    "        merged_col = df_cleaned.iloc[:, best_idx].copy()\n",
    "        first_pos = min(dup_indices)\n",
    "\n",
    "        # Drop all duplicates\n",
    "        cols_to_drop = [df_cleaned.columns[i] for i in dup_indices]\n",
    "        df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
    "\n",
    "        # Add back at first position\n",
    "        cols_list = df_cleaned.columns.tolist()\n",
    "        cols_list.insert(first_pos, dup_name)\n",
    "        df_cleaned[dup_name] = merged_col\n",
    "        df_cleaned = df_cleaned[cols_list]\n",
    "\n",
    "print(f\"After duplicate merging: {len(df_cleaned.columns)} columns, {df_cleaned.columns.nunique()} unique\")\n",
    "\n",
    "# Step 1: Consolidate Township columns\n",
    "township_cols = [col for col in df_cleaned.columns if '(Township)' in col]\n",
    "print(f\"\\nFound {len(township_cols)} Township columns\")\n",
    "\n",
    "def get_township(row):\n",
    "    for col in township_cols:\n",
    "        val = row[col]\n",
    "        if pd.notna(val) and str(val).strip():\n",
    "            return str(val).strip()\n",
    "    return None\n",
    "\n",
    "if township_cols:\n",
    "    df_cleaned['Township'] = df_cleaned.apply(get_township, axis=1)\n",
    "    print(f\"Created Township column with {df_cleaned['Township'].notna().sum()} non-null values\")\n",
    "\n",
    "    region_idx = df_cleaned.columns.tolist().index('Region')\n",
    "    cols = df_cleaned.columns.tolist()\n",
    "    cols.remove('Township')\n",
    "    cols.insert(region_idx + 1, 'Township')\n",
    "    df_cleaned = df_cleaned[cols]\n",
    "    print(f\"Positioned Township column after Region column\")\n",
    "\n",
    "    df_cleaned = df_cleaned.drop(columns=township_cols)\n",
    "    print(f\"Dropped {len(township_cols)} individual township columns\")\n",
    "\n",
    "# Step 2: Convert 0/1 to Boolean\n",
    "bool_conversion_count = 0\n",
    "for col in df_cleaned.columns:\n",
    "    try:\n",
    "        unique_vals = df_cleaned[col].dropna().unique()\n",
    "        if len(unique_vals) > 0:\n",
    "            unique_str_vals = set(str(v) for v in unique_vals)\n",
    "            if unique_str_vals.issubset({'0', '1', '0.0', '1.0'}):\n",
    "                df_cleaned[col] = df_cleaned[col].map({\n",
    "                    0: False, 1: True,\n",
    "                    0.0: False, 1.0: True,\n",
    "                    '0': False, '1': True\n",
    "                })\n",
    "                bool_conversion_count += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nConverted {bool_conversion_count} binary columns to Boolean values\")\n",
    "\n",
    "# Step 3: Remove metadata columns\n",
    "cols_to_remove = ['_id', '_uuid', '_submission_time', '_validation_status', '_notes', '_status',\n",
    "                  '_submitted_by', '_tags', '_index', 'meta/instanceID', 'start', 'end',\n",
    "                  '__version__', 'deviceid', 'subscriberid', 'simid', 'phonenumber']\n",
    "\n",
    "existing_cols_to_remove = [col for col in cols_to_remove if col in df_cleaned.columns]\n",
    "if existing_cols_to_remove:\n",
    "    df_cleaned = df_cleaned.drop(columns=existing_cols_to_remove)\n",
    "    print(f\"Removed {len(existing_cols_to_remove)} metadata columns\")\n",
    "\n",
    "# Save output file\n",
    "df_cleaned.to_csv('CLEAN_FY26.csv', index=False)\n",
    "\n",
    "print(f\"\\nCreated CLEAN_FY26.csv ({len(df_cleaned)} rows, {len(df_cleaned.columns)} columns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1z9gdz5nbdd",
   "metadata": {},
   "source": [
    "# Generate CLEAN_FY26.csv from Digital Survey Data\n",
    "\n",
    "This notebook processes the Digital Access & Behavior Survey raw data and creates a cleaned dataset for the dashboard.\n",
    "\n",
    "## Important Notes on 'None of the above' Options\n",
    "\n",
    "### Questions WITH 'None of the above':\n",
    "- **Q31** (Yetagon products heard) → **Q31_j** = \"None of the above\"\n",
    "- **Q33** (Yetagon products used) → **Q33_g** = \"None of the above\"\n",
    "\n",
    "### Questions WITHOUT 'None of the above':\n",
    "- **Q36** (Bio-product brands heard apart from Yetagon) - No \"None of the above\" option in survey\n",
    "- **Q37** (Bio-product brands used apart from Yetagon) - No \"None of the above\" option in survey\n",
    "\n",
    "The dashboard should display Q31_j and Q33_g properly. For Q36/Q37, the absence is intentional in the survey design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d0d22e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T10:45:31.893197Z",
     "iopub.status.busy": "2026-01-08T10:45:31.893126Z",
     "iopub.status.idle": "2026-01-08T10:45:31.899038Z",
     "shell.execute_reply": "2026-01-08T10:45:31.898829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Verification ===\n",
      "\n",
      "✓ Township column created successfully\n",
      "  Total townships: 558 non-null values\n",
      "  Unique townships: 117\n",
      "  ✓ Township positioned correctly after Region (positions 8, 9)\n",
      "  Sample townships: ['Aunglan', 'Kangyidaunt', 'Yamethin', 'Danubyu', 'Myothit', 'Pyapon', 'Aunglan', 'Myothit', 'Magway', 'Salin']\n",
      "\n",
      "  Top 10 townships by count:\n",
      "Township\n",
      "Myothit           26\n",
      "Mawlamyinegyun    21\n",
      "Pwintbyu          19\n",
      "Taungdwingyi      15\n",
      "Aunglan           14\n",
      "Pyawbwe           14\n",
      "Pyay              12\n",
      "Minbu             12\n",
      "Thayarwady        12\n",
      "Paungde           12\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✓ Individual township columns successfully removed\n",
      "\n",
      "✓ Q31 columns (13): ['Q31_a', 'Q31_b', 'Q31_c', 'Q31_d', 'Q31_e', 'Q31_f', 'Q31_g', 'Q31_h', 'Q31_i', 'Q31_j', 'Q31_k', 'Q31_l', 'Q31_other_text']\n",
      "  Q31_j (None of the above) present: True\n",
      "\n",
      "✓ Q33 columns (12): ['Q33_a', 'Q33_b', 'Q33_c', 'Q33_d', 'Q33_e', 'Q33_f', 'Q33_g', 'Q33_h', 'Q33_i', 'Q33_j', 'Q33_k', 'Q33_other_text']\n",
      "  Q33_g (None of the above) present: True\n",
      "\n",
      "✓ Q36 columns (17): ['Q36_a', 'Q36_b', 'Q36_c', 'Q36_d', 'Q36_e', 'Q36_f', 'Q36_g', 'Q36_h', 'Q36_i', 'Q36_j', 'Q36_k', 'Q36_l', 'Q36_m', 'Q36_n', 'Q36_o', 'Q36_other_text', 'Q36_p']\n",
      "  NOTE: Q36 does NOT have a 'None of the above' option in the original survey\n",
      "\n",
      "✓ Q37 columns (14): ['Q37_a', 'Q37_b', 'Q37_c', 'Q37_d', 'Q37_e', 'Q37_f', 'Q37_g', 'Q37_h', 'Q37_i', 'Q37_j', 'Q37_k', 'Q37_l', 'Q37_m', 'Q37_other_text']\n",
      "  NOTE: Q37 does NOT have a 'None of the above' option in the original survey\n",
      "\n",
      "Q31_j responses: 2 (0.4%) selected 'None of the above'\n",
      "Q31_j data type: object\n",
      "Q33_g responses: 77 (13.8%) selected 'None of the above'\n",
      "Q33_g data type: object\n",
      "\n",
      "=== Boolean Conversion Check ===\n",
      "✓ Q06_a: dtype=bool, unique values=[False  True]\n",
      "✓ Q06_b: dtype=bool, unique values=[False  True]\n",
      "✓ Q06_c: dtype=bool, unique values=[False  True]\n"
     ]
    }
   ],
   "source": [
    "# Verify key columns exist\n",
    "print(\"\\n=== Verification ===\")\n",
    "\n",
    "# Check Township column\n",
    "if 'Township' in df_cleaned.columns:\n",
    "    print(f\"\\n✓ Township column created successfully\")\n",
    "    print(f\"  Total townships: {df_cleaned['Township'].notna().sum()} non-null values\")\n",
    "    print(f\"  Unique townships: {df_cleaned['Township'].nunique()}\")\n",
    "    \n",
    "    # Check position relative to Region\n",
    "    region_idx = df_cleaned.columns.tolist().index('Region')\n",
    "    township_idx = df_cleaned.columns.tolist().index('Township')\n",
    "    if township_idx == region_idx + 1:\n",
    "        print(f\"  ✓ Township positioned correctly after Region (positions {region_idx}, {township_idx})\")\n",
    "    \n",
    "    # Show sample values\n",
    "    print(f\"  Sample townships: {df_cleaned['Township'].dropna().head(10).tolist()}\")\n",
    "    \n",
    "    # Show township distribution (top 10)\n",
    "    print(f\"\\n  Top 10 townships by count:\")\n",
    "    print(df_cleaned['Township'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"\\n✗ WARNING: Township column not found!\")\n",
    "\n",
    "# Verify individual township columns were removed\n",
    "township_cols_remaining = [col for col in df_cleaned.columns if '(Township)' in col]\n",
    "if len(township_cols_remaining) == 0:\n",
    "    print(f\"\\n✓ Individual township columns successfully removed\")\n",
    "else:\n",
    "    print(f\"\\n✗ WARNING: {len(township_cols_remaining)} township columns still present: {township_cols_remaining}\")\n",
    "\n",
    "# Check Q31 (should include Q31_j - None of the above)\n",
    "q31_cols = [col for col in df_cleaned.columns if col.startswith('Q31_')]\n",
    "print(f\"\\n✓ Q31 columns ({len(q31_cols)}): {sorted(q31_cols)}\")\n",
    "print(f\"  Q31_j (None of the above) present: {'Q31_j' in q31_cols}\")\n",
    "\n",
    "# Check Q33 (should include Q33_g - None of the above)\n",
    "q33_cols = [col for col in df_cleaned.columns if col.startswith('Q33_')]\n",
    "print(f\"\\n✓ Q33 columns ({len(q33_cols)}): {sorted(q33_cols)}\")\n",
    "print(f\"  Q33_g (None of the above) present: {'Q33_g' in q33_cols}\")\n",
    "\n",
    "# Check Q36 (bio-product brands heard)\n",
    "q36_cols = [col for col in df_cleaned.columns if col.startswith('Q36_')]\n",
    "print(f\"\\n✓ Q36 columns ({len(q36_cols)}): {sorted(q36_cols)}\")\n",
    "print(f\"  NOTE: Q36 does NOT have a 'None of the above' option in the original survey\")\n",
    "\n",
    "# Check Q37 (bio-product brands used)\n",
    "q37_cols = [col for col in df_cleaned.columns if col.startswith('Q37_')]\n",
    "print(f\"\\n✓ Q37 columns ({len(q37_cols)}): {sorted(q37_cols)}\")\n",
    "print(f\"  NOTE: Q37 does NOT have a 'None of the above' option in the original survey\")\n",
    "\n",
    "# Sample statistics with Boolean values\n",
    "if 'Q31_j' in df_cleaned.columns:\n",
    "    q31_j_count = (df_cleaned['Q31_j'] == True).sum()\n",
    "    q31_j_pct = (q31_j_count / len(df_cleaned) * 100) if len(df_cleaned) > 0 else 0\n",
    "    print(f\"\\nQ31_j responses: {q31_j_count} ({q31_j_pct:.1f}%) selected 'None of the above'\")\n",
    "    print(f\"Q31_j data type: {df_cleaned['Q31_j'].dtype}\")\n",
    "\n",
    "if 'Q33_g' in df_cleaned.columns:\n",
    "    q33_g_count = (df_cleaned['Q33_g'] == True).sum()\n",
    "    q33_g_pct = (q33_g_count / len(df_cleaned) * 100) if len(df_cleaned) > 0 else 0\n",
    "    print(f\"Q33_g responses: {q33_g_count} ({q33_g_pct:.1f}%) selected 'None of the above'\")\n",
    "    print(f\"Q33_g data type: {df_cleaned['Q33_g'].dtype}\")\n",
    "\n",
    "# Verify Boolean conversion\n",
    "print(\"\\n=== Boolean Conversion Check ===\")\n",
    "sample_bool_cols = [col for col in df_cleaned.columns if col.startswith('Q06_')][:3]\n",
    "for col in sample_bool_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        print(f\"✓ {col}: dtype={df_cleaned[col].dtype}, unique values={df_cleaned[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "o5tho6vumyh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T10:45:31.900022Z",
     "iopub.status.busy": "2026-01-08T10:45:31.899959Z",
     "iopub.status.idle": "2026-01-08T10:45:31.917253Z",
     "shell.execute_reply": "2026-01-08T10:45:31.917048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created CLEAN_FY26_ANSWER.csv with 241 column mappings\n",
      "\n",
      "Sample mappings:\n",
      " code                                                                                                     question          option\n",
      "  Q00                                                                                       Do you have own phone?                \n",
      "  Q01                                                                                        Farmer's phone number                \n",
      "  Q02 What is your age? (Enumerator instruction: Do NOT read options; classify response according to the options.)                \n",
      "  Q03         What is your gender? (Enumerator instruction: READ all options. Make farmer choose one option only.)                \n",
      "  Q06                                                                which crop did you grow during past 3 months?                \n",
      "Q06_a                                                                which crop did you grow during past 3 months?            Rice\n",
      "Q06_b                                                                which crop did you grow during past 3 months?          Sesame\n",
      "Q06_c                                                                which crop did you grow during past 3 months?       Groundnut\n",
      "Q06_d                                                                which crop did you grow during past 3 months?          Pulses\n",
      "Q06_e                                                                which crop did you grow during past 3 months?      Vegetables\n",
      "Q06_f                                                                which crop did you grow during past 3 months?         Flowers\n",
      "Q06_g                                                                which crop did you grow during past 3 months? Long-term trees\n",
      "Q06_h                                                                which crop did you grow during past 3 months?           Chili\n",
      "Q06_i                                                                which crop did you grow during past 3 months?          Tomato\n",
      "Q06_j                                                                which crop did you grow during past 3 months?           Onion\n"
     ]
    }
   ],
   "source": [
    "# Generate CLEAN_FY26_ANSWER.csv - mapping of column codes to questions and answers\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the raw CSV to get original column names\n",
    "df_raw = pd.read_csv('Digital_Digital_Access__Behavior_Survey_DAB_-_all_versions_-_English_eng_-_2025-12-29-02-53-07.csv', delimiter=';')\n",
    "\n",
    "# Create answer mapping\n",
    "answer_data = []\n",
    "\n",
    "for col in df_raw.columns:\n",
    "    # Special case: Q01.1 -> Q00\n",
    "    if col.startswith('Q01.1'):\n",
    "        answer_data.append({\n",
    "            'code': 'Q00',\n",
    "            'question': 'Do you have own phone?',\n",
    "            'option': ''\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # Sub-question pattern: Q##[_-]text/option\n",
    "    if match := re.match(r'(Q\\d+)[_-](.*[?\\)\"])/(.*)', col):\n",
    "        q_code = match.group(1)\n",
    "        question_text = match.group(2).strip()\n",
    "        answer_text = match.group(3).strip()\n",
    "        \n",
    "        # Find which sub-code this would be (a, b, c, etc.)\n",
    "        # We need to count how many times this Q code appeared before\n",
    "        sub_index = len([d for d in answer_data if d['code'].startswith(q_code + '_')])\n",
    "        sub_code = f\"{q_code}_{chr(97 + sub_index)}\"\n",
    "        \n",
    "        answer_data.append({\n",
    "            'code': sub_code,\n",
    "            'question': question_text,\n",
    "            'option': answer_text\n",
    "        })\n",
    "    \n",
    "    # Follow-up pattern: Q##[_-]If other...\n",
    "    elif match := re.match(r'(Q\\d+)[_-](If .*)', col):\n",
    "        q_code = match.group(1)\n",
    "        follow_up_text = match.group(2).strip()\n",
    "        follow_up_code = f\"{q_code}_other_text\"\n",
    "        \n",
    "        # Skip duplicates (some columns appear multiple times in raw data)\n",
    "        if not any(d['code'] == follow_up_code for d in answer_data):\n",
    "            answer_data.append({\n",
    "                'code': follow_up_code,\n",
    "                'question': follow_up_text,\n",
    "                'option': ''\n",
    "            })\n",
    "    \n",
    "    # Main question pattern: Q##[_-]text\n",
    "    elif match := re.match(r'(Q\\d+)[_-](.*)', col):\n",
    "        q_code = match.group(1)\n",
    "        question_text = match.group(2).strip()\n",
    "        \n",
    "        # Skip if we already have this Q code (from sub-questions or duplicates)\n",
    "        if not any(d['code'] == q_code for d in answer_data):\n",
    "            answer_data.append({\n",
    "                'code': q_code,\n",
    "                'question': question_text,\n",
    "                'option': ''\n",
    "            })\n",
    "\n",
    "# Create DataFrame and save\n",
    "df_answer = pd.DataFrame(answer_data)\n",
    "\n",
    "# Sort by code for better readability\n",
    "df_answer = df_answer.sort_values('code').reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "df_answer.to_csv('CLEAN_FY26_ANSWER.csv', index=False)\n",
    "\n",
    "print(f\"Created CLEAN_FY26_ANSWER.csv with {len(df_answer)} column mappings\")\n",
    "print(f\"\\nSample mappings:\")\n",
    "print(df_answer.head(15).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Proximity",
   "language": "python",
   "name": "proximity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
